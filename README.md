# ðŸ§  LLMs-for-Text_Summarization

This repository explores the use of **Large Language Models (LLMs)** for **text summarization**â€”both **abstractive** and **extractive**â€”using state-of-the-art transformer architectures.

---

## ðŸš€ Project Overview

Text summarization is a core task in Natural Language Processing (NLP) that aims to shorten long documents while preserving their key information. This project implements and evaluates powerful LLMs to generate summaries from various datasets.

### ðŸ“Œ Key Objectives:
- Implement LLMs for text summarization
- Evaluate models on benchmark datasets
- Compare abstractive vs extractive approaches
- Explore multilingual or domain-specific summarization

---

## ðŸ§  Models Used

- **BERT** (for extractive summarization)
- **T5** (Text-to-Text Transfer Transformer)
- **BART**
- **PEGASUS**
- **GPT-based models**

---

## ðŸ“š Datasets
- CNN / DailyMail
- XSum
- Multi-News
- [Optional] Custom Urdu News Corpu

## ðŸ“Š Evaluation Metrics
- ROUGE-1, ROUGE-2, ROUGE-L
- BLEU
- BERTScore
- Human Evaluation (optional)

## ðŸ“ˆ Features
âœ… Abstractive and extractive summarization
âœ… HuggingFace Transformers integration
âœ… ROUGE/BLEU evaluation
âœ… Fine-tuning on custom data
âœ… Multilingual support (optional)
âœ… Visualization of summaries



